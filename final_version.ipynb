{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c130d072-1fee-4b84-a23e-f2f1f4f8428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from rich.progress import track\n",
    "\n",
    "import json\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d147e9-c039-4d16-a096-29f4365d6c47",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51afcaf-dc8e-434e-96b4-206293a4e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading CSV data from OSF file ---\n",
      "Found 10 CSV files. Loading each one...\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2016.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2017.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2018.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2019.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2020.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2021.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2022.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2023.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2024.csv\n",
      "  - Successfully loaded and parsed dates from hrl_load_metered_2025.csv\n",
      "\n",
      "Successfully loaded 10 DataFrames.\n",
      "\n",
      "--- 2. Fetching Weather Data for CSV Date Range ---\n",
      "Date range found in CSVs: 2016-01-01 to 2025-11-17\n",
      "Fetching corresponding weather data from Open-Meteo...\n",
      "Successfully fetched and parsed weather data.\n",
      "\n",
      "--- 3. Processing and Merging Weather Data ---\n",
      "Created weather DataFrame. Head:\n",
      "                     temperature_2m\n",
      "time                               \n",
      "2016-01-01 00:00:00             7.0\n",
      "2016-01-01 01:00:00             6.3\n",
      "2016-01-01 02:00:00             5.6\n",
      "2016-01-01 03:00:00             5.1\n",
      "2016-01-01 04:00:00             4.7\n",
      "\n",
      "Example merge with first file: 'hrl_load_metered_2016.csv'\n",
      "Merged DataFrame (head):\n",
      "                    datetime_beginning_ept nerc_region mkt_region zone  \\\n",
      "2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL   AE   \n",
      "2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL   BC   \n",
      "2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL  DPL   \n",
      "2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL  DPL   \n",
      "2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL   JC   \n",
      "\n",
      "                    load_area        mw  is_verified  temperature_2m  \n",
      "2016-01-01 05:00:00        AE   844.707         True             4.4  \n",
      "2016-01-01 05:00:00        BC  3008.152         True             4.4  \n",
      "2016-01-01 05:00:00     DPLCO  1707.194         True             4.4  \n",
      "2016-01-01 05:00:00    EASTON    24.611         True             4.4  \n",
      "2016-01-01 05:00:00        JC  2176.022         True             4.4  \n",
      "Created weather DataFrame. Head:\n",
      "                     temperature_2m\n",
      "time                               \n",
      "2016-01-01 00:00:00             7.0\n",
      "2016-01-01 01:00:00             6.3\n",
      "2016-01-01 02:00:00             5.6\n",
      "2016-01-01 03:00:00             5.1\n",
      "2016-01-01 04:00:00             4.7\n",
      "\n",
      "Merging weather with all CSVs...\n",
      "  - merged hrl_load_metered_2016.csv: 245,952 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2017.csv: 250,417 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2018.csv: 254,784 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2019.csv: 262,800 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2020.csv: 263,520 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2021.csv: 262,800 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2022.csv: 262,800 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2023.csv: 262,800 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2024.csv: 263,520 rows (missing temperature_2m: 0)\n",
      "  - merged hrl_load_metered_2025.csv: 11,550 rows (missing temperature_2m: 0)\n",
      "\n",
      "Merged weather into 10 files.\n",
      "\n",
      "Combined merged_df shape: (2340943, 10)\n",
      "            timestamp datetime_beginning_ept nerc_region mkt_region zone  \\\n",
      "0 2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL   AE   \n",
      "1 2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL   BC   \n",
      "2 2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL  DPL   \n",
      "3 2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL  DPL   \n",
      "4 2016-01-01 05:00:00   1/1/2016 12:00:00 AM         RFC     MIDATL   JC   \n",
      "\n",
      "  load_area        mw  is_verified  temperature_2m                source_file  \n",
      "0        AE   844.707         True             4.4  hrl_load_metered_2016.csv  \n",
      "1        BC  3008.152         True             4.4  hrl_load_metered_2016.csv  \n",
      "2     DPLCO  1707.194         True             4.4  hrl_load_metered_2016.csv  \n",
      "3    EASTON    24.611         True             4.4  hrl_load_metered_2016.csv  \n",
      "4        JC  2176.022         True             4.4  hrl_load_metered_2016.csv  \n",
      "\n",
      "--- Python script execution finished. ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading CSV data from OSF file ---\")\n",
    "DATA_DIR = 'hrl_load_metered_2016-2025/'\n",
    "search_path = os.path.join(DATA_DIR, '**', '*.csv')\n",
    "csv_files = sorted(glob.glob(search_path, recursive=True))\n",
    "if not csv_files:\n",
    "    print(f\"Error: No CSV files found in {DATA_DIR}.\")\n",
    "    dataframes = {}\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Loading each one...\")\n",
    "    dataframes = {}\n",
    "    all_dates = []  \n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            date_col = df.columns[0]\n",
    "            parsed_dates = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            all_dates.append(parsed_dates)\n",
    "            dataframes[file_name] = df\n",
    "            print(f\"  - Successfully loaded and parsed dates from {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {file_name}: {e}\")\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded {len(dataframes)} DataFrames.\")\n",
    "\n",
    "#  --weather\n",
    "if not dataframes:\n",
    "    print(\"\\nNo dataframes loaded, skipping weather fetch.\")\n",
    "else:\n",
    "    print(\"\\n--- 2. Fetching Weather Data for CSV Date Range ---\")\n",
    "    all_dates_combined = pd.concat(all_dates).dropna()\n",
    "    min_date = all_dates_combined.min()\n",
    "    max_date = all_dates_combined.max()\n",
    "    \n",
    "    # Format dates for the API (YYYY-MM-DD)\n",
    "    start_str = min_date.strftime('%Y-%m-%d')\n",
    "    end_str = max_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Date range found in CSVs: {start_str} to {end_str}\")\n",
    "    print(\"Fetching corresponding weather data from Open-Meteo...\")\n",
    "\n",
    "    API_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": 39.95,   # Philadelphia\n",
    "        \"longitude\": -75.16, # Philadelphia\n",
    "        \"start_date\": start_str,\n",
    "        \"end_date\": end_str,\n",
    "        \"hourly\": \"temperature_2m\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params)\n",
    "        response.raise_for_status() \n",
    "        weather_data = response.json()\n",
    "        print(\"Successfully fetched and parsed weather data.\")\n",
    "        print(\"\\n--- 3. Processing and Merging Weather Data ---\")\n",
    "        hourly_data = weather_data.get('hourly', {})\n",
    "        if not hourly_data:\n",
    "            print(\"Error: No 'hourly' data found in API response.\")\n",
    "        else:\n",
    "            # Create a clean, time-indexed DataFrame for the weather\n",
    "            weather_df = pd.DataFrame(hourly_data)\n",
    "            weather_df['time'] = pd.to_datetime(weather_df['time'])\n",
    "            weather_df = weather_df.set_index('time')\n",
    "            print(\"Created weather DataFrame. Head:\")\n",
    "            print(weather_df.head())\n",
    "\n",
    "            # Now, let's merge this weather data back into your original CSVs\n",
    "            # We'll just show an example using the first DataFrame\n",
    "            \n",
    "            first_df_key = list(dataframes.keys())[0]\n",
    "            print(f\"\\nExample merge with first file: '{first_df_key}'\")\n",
    "            \n",
    "            original_df = dataframes[first_df_key]\n",
    "            date_col = original_df.columns[0]\n",
    "            \n",
    "            # Parse dates and set as index (this time on the original df)\n",
    "            original_df[date_col] = pd.to_datetime(original_df[date_col])\n",
    "            original_df = original_df.set_index(date_col)\n",
    "            \n",
    "            # Merge! This joins the temperature to the matching timestamp.\n",
    "            # 'how=left' keeps all your original data.\n",
    "            merged_df = original_df.merge(weather_df, left_index=True, right_index=True, how='left')\n",
    "            \n",
    "            print(\"Merged DataFrame (head):\")\n",
    "            print(merged_df.head())\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Could not decode JSON response from weather API.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during weather processing: {e}\")\n",
    "# --- 3. Processing and Merging Weather Data (for ALL CSVs) ---\n",
    "\n",
    "hourly_data = weather_data.get('hourly', {})\n",
    "if not hourly_data:\n",
    "    print(\"Error: No 'hourly' data found in API response.\")\n",
    "else:\n",
    "    # 3a) Clean, index the weather\n",
    "    weather_df = pd.DataFrame(hourly_data)\n",
    "    weather_df['time'] = pd.to_datetime(weather_df['time'], errors='coerce')\n",
    "    weather_df = weather_df.dropna(subset=['time']).set_index('time').sort_index()\n",
    "    print(\"Created weather DataFrame. Head:\")\n",
    "    print(weather_df.head())\n",
    "\n",
    "    # 3b) Merge weather into EVERY loaded CSV\n",
    "    merged_dataframes = {}\n",
    "    print(\"\\nMerging weather with all CSVs...\")\n",
    "    for fname, original_df in dataframes.items():\n",
    "        df_i = original_df.copy()\n",
    "        date_col = df_i.columns[0]                      # assume first column is the timestamp\n",
    "        df_i[date_col] = pd.to_datetime(df_i[date_col], errors='coerce')\n",
    "        df_i = df_i.dropna(subset=[date_col]).set_index(date_col).sort_index()\n",
    "\n",
    "        merged_i = df_i.merge(weather_df, left_index=True, right_index=True, how='left')\n",
    "        merged_i[\"source_file\"] = fname                 # keep provenance if you want\n",
    "        merged_dataframes[fname] = merged_i\n",
    "\n",
    "        missing = int(merged_i[\"temperature_2m\"].isna().sum())\n",
    "        print(f\"  - merged {fname}: {len(merged_i):,} rows (missing temperature_2m: {missing:,})\")\n",
    "\n",
    "    print(f\"\\nMerged weather into {len(merged_dataframes)} files.\")\n",
    "\n",
    "    # 3c) (Optional) Concatenate into one big DataFrame\n",
    "    merged_df = (\n",
    "        pd.concat(merged_dataframes.values(), axis=0, ignore_index=False)\n",
    "          .rename_axis(\"timestamp\")                    # index name for the merged time column\n",
    "          .reset_index()\n",
    "    )\n",
    "    print(\"\\nCombined merged_df shape:\", merged_df.shape)\n",
    "    print(merged_df.head())\n",
    "print(\"\\n--- Python script execution finished. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "708a1798-49be-4b76-8cc1-084796d1a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_area = [\n",
    "    \"AECO\",\n",
    "    \"AEPAPT\",\n",
    "    \"AEPIMP\",\n",
    "    \"AEPKPT\",\n",
    "    \"AEPOPT\",\n",
    "    \"AP\",\n",
    "    \"BC\",\n",
    "    \"CE\",\n",
    "    \"DAY\",\n",
    "    \"DEOK\",\n",
    "    \"DOM\",\n",
    "    \"DPLCO\",\n",
    "    \"DUQ\",\n",
    "    \"EASTON\",\n",
    "    \"EKPC\",\n",
    "    \"JC\",\n",
    "    \"ME\",\n",
    "    \"OE\",\n",
    "    \"OVEC\",\n",
    "    \"PAPWR\",\n",
    "    \"PE\",\n",
    "    \"PEPCO\",\n",
    "    \"PLCO\",\n",
    "    \"PN\",\n",
    "    \"PS\",\n",
    "    \"RECO\",\n",
    "    \"SMECO\",\n",
    "    \"UGI\",\n",
    "    \"VMEU\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4624187-815d-46e4-8797-7dc40df4d1d0",
   "metadata": {},
   "source": [
    "## Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec3f0bd5-d3b5-438c-81ef-7a193f5e840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9da0784-c494-480a-8c2e-530c1b730e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_94108/3509989948.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"ts\"] = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows after leakage sanitization: 2,392,351\n",
      "\n",
      "Validation (NO validation info used in train or features): 2025-10-22..2025-10-31\n",
      "  RMSE: 233.73 MW\n",
      "  MAE : 137.63 MW\n",
      "  MAPE: 5.46%\n"
     ]
    }
   ],
   "source": [
    "df = merged_df.copy()\n",
    "if 'load_area' in globals():\n",
    "    df = df[df[\"load_area\"].isin(load_area)].copy()\n",
    "\n",
    "# timestamps (EPT wall-clock)\n",
    "df[\"ts\"] = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ts\"]).sort_values([\"load_area\", \"ts\"]).reset_index(drop=True)\n",
    "df[\"year\"] = df[\"ts\"].dt.year\n",
    "df[\"hour\"] = df[\"ts\"].dt.hour\n",
    "df[\"doy\"]  = df[\"ts\"].dt.dayofyear  \n",
    "\n",
    "def add_last_year_feats(frame: pd.DataFrame, n_years: int = 4) -> pd.DataFrame:\n",
    "    out = frame\n",
    "    base = (frame[[\"load_area\", \"ts\", \"mw\"]]\n",
    "            .groupby([\"load_area\", \"ts\"], as_index=False)[\"mw\"].mean())  # collapse any DST dupes\n",
    "    for k in range(1, n_years + 1):\n",
    "        prev = base.rename(columns={\"mw\": f\"mw_ly{k}\"}).copy()\n",
    "        prev[\"ts\"] = prev[\"ts\"] + pd.DateOffset(years=k)  # align prior-year load to current timestamp\n",
    "        out = out.merge(prev, on=[\"load_area\", \"ts\"], how=\"left\")\n",
    "    return out\n",
    "\n",
    "df = add_last_year_feats(df, n_years=4)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) Create lag features **by exact timestamp lookup** (not shift),\n",
    "#    so we can sanitize training rows that would reference holdout.\n",
    "# -------------------------\n",
    "def add_lag_by_lookup(frame: pd.DataFrame, hours: int, out_col: str) -> pd.DataFrame:\n",
    "    key = frame[[\"load_area\", \"ts\", \"mw\"]].copy()\n",
    "    key = key.rename(columns={\"mw\": out_col})\n",
    "    key[\"ts\"] = key[\"ts\"] + pd.Timedelta(hours=hours)  # align t-hours -> t\n",
    "    return frame.merge(key, on=[\"load_area\", \"ts\"], how=\"left\")\n",
    "\n",
    "for h, c in [(24, \"mw_lag24\"), (48, \"mw_lag48\"), (168, \"mw_lag168\")]:\n",
    "    df = add_lag_by_lookup(df, hours=h, out_col=c)\n",
    "# Define the 10‑day holdout window\n",
    "HOLD_LO = pd.Timestamp(\"2025-10-22 00:00:00\")\n",
    "HOLD_HI = pd.Timestamp(\"2025-11-01 00:00:00\")  # exclusive\n",
    "is_holdout = (df[\"ts\"].ge(HOLD_LO) & df[\"ts\"].lt(HOLD_HI) & df[\"year\"].eq(2025))\n",
    "\n",
    "# Split (no braces, no dict keys, just a boolean Series aligned to df.index)\n",
    "valid_df = df.loc[is_holdout].copy()\n",
    "train_df = df.loc[~is_holdout].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 4) SANITIZE TRAIN: any lag whose reference time falls inside holdout is set to NaN.\n",
    "#    (Prevents leakage of validation actuals into training features.)\n",
    "# -------------------------\n",
    "for lag_h, lag_col in [(24, \"mw_lag24\"), (48, \"mw_lag48\"), (168, \"mw_lag168\")]:\n",
    "    ref_ts = train_df[\"ts\"] - pd.Timedelta(hours=lag_h)\n",
    "    leak_mask = (ref_ts >= HOLD_LO) & (ref_ts < HOLD_HI)\n",
    "    train_df.loc[leak_mask, lag_col] = np.nan\n",
    "\n",
    "# We require all lag features + mw_ly1..mw_ly4 to exist for training\n",
    "feat_cols = [\"mw_lag24\", \"mw_lag48\", \"mw_lag168\"]\n",
    "train_df = train_df.dropna(subset=feat_cols + [\"mw\"]).copy()\n",
    "\n",
    "print(f\"Train rows after leakage sanitization: {len(train_df):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Fit boosting model (no CV; uses internal early stopping on TRAIN only)\n",
    "# -------------------------\n",
    "X_train = train_df[feat_cols].to_numpy()\n",
    "y_train = train_df[\"mw\"].to_numpy()\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    learning_rate=0.05,\n",
    "    max_iter=2000,\n",
    "    early_stopping=False,     # TRAIN-only internal holdout\n",
    "    n_iter_no_change=200,\n",
    "    validation_fraction=0.1, # fraction of TRAIN\n",
    "    max_bins=255,\n",
    "    min_samples_leaf=50,\n",
    "    l2_regularization=1.0,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------\n",
    "# 6) PREDICT VALIDATION **without using validation actuals**:\n",
    "#    Roll forward per (area, ts): if a required lag points inside holdout,\n",
    "#    use our prior PREDICTION for that lag time (never the actual).\n",
    "# -------------------------\n",
    "# Known dictionary: all training actuals only\n",
    "known = {(r.load_area, r.ts): r.mw for r in train_df[[\"load_area\",\"ts\",\"mw\"]].itertuples(index=False)}\n",
    "\n",
    "# Prepare container for predictions\n",
    "valid_df = valid_df.sort_values([\"load_area\", \"ts\"]).copy()\n",
    "valid_df[\"pred\"] = np.nan\n",
    "\n",
    "def row_feat_vector(row, known_map):\n",
    "    area, ts = row[\"load_area\"], row[\"ts\"]\n",
    "    l24  = known_map.get((area, ts - pd.Timedelta(hours=24)),  np.nan)\n",
    "    l48  = known_map.get((area, ts - pd.Timedelta(hours=48)),  np.nan)\n",
    "    l168 = known_map.get((area, ts - pd.Timedelta(hours=168)), np.nan)\n",
    "    return np.array([[l24, l48, l168]])\n",
    "\n",
    "# Roll forward within each area\n",
    "for area, g in valid_df.groupby(\"load_area\", sort=False):\n",
    "    idxs = g.sort_values(\"ts\").index\n",
    "    for idx in idxs:\n",
    "        feats = row_feat_vector(valid_df.loc[idx], known)\n",
    "        yhat  = float(hgb.predict(feats)[0])\n",
    "        valid_df.at[idx, \"pred\"] = yhat\n",
    "        # update knowledge so downstream lags can use our prediction\n",
    "        known[(area, valid_df.at[idx, \"ts\"])] = yhat\n",
    "\n",
    "# -------------------------\n",
    "# 7) Metrics on the 10-day validation window\n",
    "# -------------------------\n",
    "y_valid = valid_df[\"mw\"].to_numpy()\n",
    "y_hat   = valid_df[\"pred\"].to_numpy()\n",
    "\n",
    "print(\"\\nValidation (NO validation info used in train or features): 2025-10-22..2025-10-31\")\n",
    "print(f\"  RMSE: {rmse(y_valid, y_hat):,.2f} MW\")\n",
    "print(f\"  MAE : {mae(y_valid,  y_hat):,.2f} MW\")\n",
    "print(f\"  MAPE: {mape(y_valid, y_hat):,.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca0aca-a6a3-4dc2-bbfc-1e70c99b8073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b59a05ca-c549-45e5-891c-223065e88949",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516ea883-fd27-411c-81b8-45af6c8f1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Peak-hour ±1 accuracy on last 10 days (all 29 areas):\n",
      "  Raw argmax of forecast:   59.1%\n",
      "  Soft-argmax (kernel [1,3,1]):  59.9%\n",
      "  Probabilistic selector:    57.1%\n",
      "\n",
      "Per-area ±1 accuracy (10-day window):\n",
      "           acc_argmax  acc_soft  acc_prob\n",
      "load_area                                \n",
      "AECO         1.000000  1.000000  1.000000\n",
      "PS           1.000000  1.000000  1.000000\n",
      "JC           1.000000  1.000000  1.000000\n",
      "CE           0.888889  0.888889  0.888889\n",
      "RECO         1.000000  1.000000  0.888889\n",
      "VMEU         0.888889  0.888889  0.888889\n",
      "BC           0.777778  0.777778  0.777778\n",
      "DOM          0.555556  0.777778  0.777778\n",
      "EASTON       0.777778  0.777778  0.777778\n",
      "SMECO        0.777778  0.777778  0.666667\n",
      "DPLCO        0.666667  0.666667  0.666667\n",
      "PE           0.666667  0.666667  0.666667\n",
      "PAPWR        0.555556  0.555556  0.555556\n",
      "AEPKPT       0.666667  0.555556  0.555556\n",
      "PN           0.555556  0.666667  0.555556\n",
      "AEPAPT       0.444444  0.555556  0.555556\n",
      "PLCO         0.444444  0.444444  0.444444\n",
      "PEPCO        0.444444  0.444444  0.444444\n",
      "EKPC         0.444444  0.444444  0.444444\n",
      "ME           0.555556  0.555556  0.444444\n",
      "AP           0.333333  0.333333  0.333333\n",
      "AEPOPT       0.444444  0.333333  0.333333\n",
      "DEOK         0.444444  0.444444  0.333333\n",
      "AEPIMP       0.222222  0.333333  0.333333\n",
      "DUQ          0.222222  0.222222  0.222222\n",
      "OE           0.444444  0.333333  0.222222\n",
      "OVEC         0.000000  0.000000  0.111111\n",
      "DAY          0.333333  0.333333  0.111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_94108/2306562405.py:133: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda d: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# 1) Get training residual scale (NO validation info)\n",
    "# -------------------------\n",
    "# Predict on the final training set to compute residuals\n",
    "X_tr = train_df[[\"mw_lag24\", \"mw_lag48\", \"mw_lag168\"]].to_numpy()\n",
    "y_tr = train_df[\"mw\"].to_numpy()\n",
    "train_df = train_df.copy()\n",
    "train_df[\"pred_tr\"] = hgb.predict(X_tr)\n",
    "train_df[\"resid\"] = train_df[\"mw\"] - train_df[\"pred_tr\"]\n",
    "\n",
    "# Robust hourly sigma per (area, hour): 1.4826 * MAD\n",
    "def robust_sigma(s):\n",
    "    med = np.median(s)\n",
    "    mad = np.median(np.abs(s - med))\n",
    "    if np.isnan(mad) or mad == 0:\n",
    "        # fallback to std or small floor to avoid zero variance\n",
    "        std = np.std(s)\n",
    "        return float(std if std > 1e-6 else 1.0)\n",
    "    return float(1.4826 * mad)\n",
    "\n",
    "sigma_map = (\n",
    "    train_df.groupby([\"load_area\", \"hour\"], observed=True)[\"resid\"]\n",
    "            .apply(robust_sigma)\n",
    ")\n",
    "\n",
    "# Global hour-only fallback if a (area,hour) combo is missing\n",
    "sigma_hour_global = (\n",
    "    train_df.groupby(\"hour\", observed=True)[\"resid\"]\n",
    "            .apply(robust_sigma)\n",
    ")\n",
    "\n",
    "def get_sigma_vec(area, hours):\n",
    "    \"\"\"Return a vector of sigma values for the given area and iterable of hours.\"\"\"\n",
    "    sig = []\n",
    "    for h in hours:\n",
    "        key = (area, int(h))\n",
    "        if key in sigma_map.index:\n",
    "            sig.append(float(sigma_map.loc[key]))\n",
    "        else:\n",
    "            sig.append(float(sigma_hour_global.loc[int(h)]))\n",
    "    return np.array(sig, dtype=float)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Peak-hour selectors\n",
    "# -------------------------\n",
    "def soft_argmax(mu, kernel=(1, 3, 1)):\n",
    "    \"\"\"Argmax after small smoothing kernel (aligned with ±1 success).\"\"\"\n",
    "    k = np.array(kernel, dtype=float)\n",
    "    sm = np.convolve(mu, k, mode=\"same\")\n",
    "    return int(np.argmax(sm))\n",
    "\n",
    "def probabilistic_peak(mu, sigma, n_draws=1000, rng=None):\n",
    "    \"\"\"\n",
    "    Monte-Carlo selector maximizing Pr(|ĥ − peak| ≤ 1).\n",
    "    mu: (24,) mean vector for the day\n",
    "    sigma: (24,) std vector (from training residuals)\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    # draw iid normals; (n_draws, 24)\n",
    "    Z = rng.standard_normal((n_draws, len(mu)))\n",
    "    Y = mu[None, :] + sigma[None, :] * Z\n",
    "\n",
    "    # index of max per draw\n",
    "    hmax = np.argmax(Y, axis=1)\n",
    "\n",
    "    # vote for (hmax-1,hmax,hmax+1)\n",
    "    votes = np.zeros(len(mu), dtype=int)\n",
    "    for h in hmax:\n",
    "        votes[h] += 1\n",
    "        if h - 1 >= 0: votes[h - 1] += 1\n",
    "        if h + 1 < len(mu): votes[h + 1] += 1\n",
    "    return int(np.argmax(votes))\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build per-day, per-area predictions and evaluate\n",
    "# -------------------------\n",
    "# Keep only the last-10-days holdout (valid_df) and ensure we have all 24 hours per day\n",
    "valid_df = valid_df.copy()\n",
    "valid_df[\"date\"] = valid_df[\"ts\"].dt.date\n",
    "\n",
    "rows = []\n",
    "for (area, day), g in valid_df.groupby([\"load_area\", \"date\"], sort=True):\n",
    "    g = g.sort_values(\"hour\")\n",
    "    if g[\"hour\"].nunique() < 24:\n",
    "        # skip partial days (shouldn't occur in this window, but be safe)\n",
    "        continue\n",
    "\n",
    "    mu = g[\"pred\"].to_numpy()      # our forecast means for that day\n",
    "    sig = get_sigma_vec(area, g[\"hour\"].to_numpy())\n",
    "    y  = g[\"mw\"].to_numpy()\n",
    "\n",
    "    # true peak hour\n",
    "    h_true = int(np.argmax(y))\n",
    "\n",
    "    # selectors\n",
    "    h_argmax  = int(np.argmax(mu))\n",
    "    h_soft    = soft_argmax(mu, kernel=(1, 3, 1))\n",
    "    h_prob    = probabilistic_peak(mu, sig, n_draws=1000)\n",
    "\n",
    "    rows.append({\n",
    "        \"load_area\": area,\n",
    "        \"date\": day,\n",
    "        \"peak_true\": h_true,\n",
    "        \"peak_argmax\": h_argmax,\n",
    "        \"peak_soft\": h_soft,\n",
    "        \"peak_prob\": h_prob\n",
    "    })\n",
    "\n",
    "ph_df = pd.DataFrame(rows).sort_values([\"load_area\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "def acc_pm1(pred, truth):\n",
    "    pred = np.asarray(pred, dtype=int)\n",
    "    truth = np.asarray(truth, dtype=int)\n",
    "    return float(np.mean(np.abs(pred - truth) <= 1))\n",
    "\n",
    "# Overall ±1 accuracy\n",
    "acc_argmax = acc_pm1(ph_df[\"peak_argmax\"], ph_df[\"peak_true\"])\n",
    "acc_soft   = acc_pm1(ph_df[\"peak_soft\"],   ph_df[\"peak_true\"])\n",
    "acc_prob   = acc_pm1(ph_df[\"peak_prob\"],   ph_df[\"peak_true\"])\n",
    "\n",
    "print(\"\\nPeak-hour ±1 accuracy on last 10 days (all 29 areas):\")\n",
    "print(f\"  Raw argmax of forecast:  {acc_argmax*100:5.1f}%\")\n",
    "print(f\"  Soft-argmax (kernel [1,3,1]): {acc_soft*100:5.1f}%\")\n",
    "print(f\"  Probabilistic selector:   {acc_prob*100:5.1f}%\")\n",
    "\n",
    "# (Optional) accuracy per area\n",
    "per_area = (\n",
    "    ph_df.groupby(\"load_area\", observed=True)\n",
    "         .apply(lambda d: pd.Series({\n",
    "             \"acc_argmax\": acc_pm1(d[\"peak_argmax\"], d[\"peak_true\"]),\n",
    "             \"acc_soft\":   acc_pm1(d[\"peak_soft\"],   d[\"peak_true\"]),\n",
    "             \"acc_prob\":   acc_pm1(d[\"peak_prob\"],   d[\"peak_true\"]),\n",
    "         }))\n",
    "         .sort_values(\"acc_prob\", ascending=False)\n",
    ")\n",
    "print(\"\\nPer-area ±1 accuracy (10-day window):\")\n",
    "print(per_area)\n",
    "\n",
    "# If you want the chosen peak to submit, pick the strongest selector:\n",
    "ph_df[\"peak_submit\"] = ph_df[\"peak_prob\"]   # or \"peak_soft\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c99dc-bf68-4367-b205-4e33b70dec94",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ec6138-5de5-4f47-9d90-a232b08a6733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_94108/2442827528.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"ts\"]   = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024→2025 week | TOP-3 baseline]  Areas=28\n",
      "  Total cost = 101   |   FN = 20   FP = 21   |   Hit@2 = 64.29%\n",
      "\n",
      "Per-area (Hit@2, FP, FN, Cost):\n",
      "           Hit@2  FP  FN  Cost\n",
      "load_area                     \n",
      "AEPIMP         2   0   0     0\n",
      "AEPOPT         2   0   0     0\n",
      "PLCO           2   0   0     0\n",
      "CE             2   0   0     0\n",
      "DUQ            2   0   0     0\n",
      "OVEC           2   0   0     0\n",
      "ME             2   0   0     0\n",
      "DPLCO          2   1   0     1\n",
      "OE             2   1   0     1\n",
      "EASTON         1   1   1     5\n",
      "RECO           1   1   1     5\n",
      "PS             1   1   1     5\n",
      "PN             1   1   1     5\n",
      "PEPCO          1   1   1     5\n",
      "PE             1   1   1     5\n",
      "PAPWR          1   1   1     5\n",
      "VMEU           1   1   1     5\n",
      "EKPC           1   1   1     5\n",
      "SMECO          1   1   1     5\n",
      "DOM            1   1   1     5\n",
      "DEOK           1   1   1     5\n",
      "DAY            1   1   1     5\n",
      "BC             1   1   1     5\n",
      "AP             1   1   1     5\n",
      "AEPKPT         1   1   1     5\n",
      "AEPAPT         1   1   1     5\n",
      "JC             1   1   1     5\n",
      "AECO           0   1   2     9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_94108/2442827528.py:100: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_area_stats).sort_values(\"Cost\", ascending=True))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Build daily-peak table from your merged_df\n",
    "# ------------------------------------------------------------\n",
    "df = merged_df.copy()\n",
    "if 'load_area' in globals():\n",
    "    df = df[df[\"load_area\"].isin(load_area)].copy()\n",
    "\n",
    "df[\"ts\"]   = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n",
    "df         = df.dropna(subset=[\"ts\"]).sort_values([\"load_area\",\"ts\"]).reset_index(drop=True)\n",
    "df[\"date\"] = df[\"ts\"].dt.normalize()\n",
    "\n",
    "# Actual daily peak MW\n",
    "daily = (df.groupby([\"load_area\",\"date\"], observed=True)[\"mw\"]\n",
    "           .max().rename(\"mw_peak\").reset_index())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Define corresponding Mon–Sun weeks around Oct 31\n",
    "# ------------------------------------------------------------\n",
    "K_PAST = 3  # choose top-3 from 2024 to project onto 2025\n",
    "\n",
    "CENTER_2024 = pd.Timestamp(\"2024-10-31\")\n",
    "W24_START   = CENTER_2024.to_period(\"W-MON\").start_time.normalize()\n",
    "W24_END     = W24_START + pd.Timedelta(days=7)\n",
    "\n",
    "CENTER_2025 = pd.Timestamp(\"2025-10-31\")\n",
    "W25_START   = CENTER_2025.to_period(\"W-MON\").start_time.normalize()\n",
    "W25_END     = W25_START + pd.Timedelta(days=7)\n",
    "\n",
    "# Slice to those weeks\n",
    "d24 = daily[(daily[\"date\"] >= W24_START) & (daily[\"date\"] < W24_END)].copy()\n",
    "d25 = daily[(daily[\"date\"] >= W25_START) & (daily[\"date\"] < W25_END)].copy()\n",
    "\n",
    "# Keep only areas that exist in BOTH weeks\n",
    "areas_24 = set(d24[\"load_area\"].unique())\n",
    "areas_25 = set(d25[\"load_area\"].unique())\n",
    "common_areas = areas_24 & areas_25\n",
    "d24 = d24[d24[\"load_area\"].isin(common_areas)].copy()\n",
    "d25 = d25[d25[\"load_area\"].isin(common_areas)].copy()\n",
    "\n",
    "# Week positions: 0..6 from the Monday start (more robust than names)\n",
    "week24_pos = pd.DataFrame({\"date\": [W24_START + pd.Timedelta(days=i) for i in range(7)], \"pos\": np.arange(7)})\n",
    "week25_pos = pd.DataFrame({\"date\": [W25_START + pd.Timedelta(days=i) for i in range(7)], \"pos\": np.arange(7)})\n",
    "\n",
    "d24 = d24.merge(week24_pos, on=\"date\", how=\"left\")\n",
    "d25 = d25.merge(week25_pos, on=\"date\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Past-year TOP-3 argmax per area (on 2024 week) → map to 2025 week positions\n",
    "# ------------------------------------------------------------\n",
    "topK_2024 = (d24.sort_values([\"load_area\",\"mw_peak\"], ascending=[True, False])\n",
    "               .groupby(\"load_area\", as_index=False).head(K_PAST)\n",
    "               [[\"load_area\",\"pos\"]]\n",
    "               .groupby(\"load_area\")[\"pos\"].apply(lambda s: sorted(pd.unique(s)))\n",
    "               .rename(\"pos_set\").reset_index())\n",
    "\n",
    "# Predict 2025 peak days: those with positions in last year's TOP-3 set\n",
    "d25 = d25.merge(topK_2024, on=\"load_area\", how=\"left\")\n",
    "d25[\"pred_peakday\"] = d25.apply(lambda r: int(isinstance(r[\"pos_set\"], list) and (r[\"pos\"] in r[\"pos_set\"])), axis=1)\n",
    "pred_days = d25[d25[\"pred_peakday\"] == 1][[\"load_area\",\"date\",\"pred_peakday\"]]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Ground truth in the 2025 week: top-2 actual days by daily peak\n",
    "# ------------------------------------------------------------\n",
    "true_top2 = (d25.sort_values([\"load_area\",\"mw_peak\"], ascending=[True, False])\n",
    "               .groupby(\"load_area\", as_index=False).head(2)\n",
    "               .assign(y_true=1)[[\"load_area\",\"date\",\"y_true\"]])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Evaluate with FN=4, FP=1 and report Hit@2\n",
    "# ------------------------------------------------------------\n",
    "eval_df = (d25[[\"load_area\",\"date\"]]\n",
    "           .merge(true_top2, on=[\"load_area\",\"date\"], how=\"left\")\n",
    "           .merge(pred_days, on=[\"load_area\",\"date\"], how=\"left\")\n",
    "           .fillna({\"y_true\":0, \"pred_peakday\":0})\n",
    "           .sort_values([\"load_area\",\"date\"]).reset_index(drop=True))\n",
    "\n",
    "FN = int(((eval_df[\"y_true\"]==1) & (eval_df[\"pred_peakday\"]==0)).sum())\n",
    "FP = int(((eval_df[\"y_true\"]==0) & (eval_df[\"pred_peakday\"]==1)).sum())\n",
    "TP = int(((eval_df[\"y_true\"]==1) & (eval_df[\"pred_peakday\"]==1)).sum())\n",
    "\n",
    "areas = eval_df[\"load_area\"].nunique()\n",
    "cost_total = 4*FN + FP\n",
    "hit_at_2   = TP / max(1, 2*areas)  # two true peak days per area\n",
    "\n",
    "print(f\"[2024→2025 week | TOP-{K_PAST} baseline]  Areas={areas}\")\n",
    "print(f\"  Total cost = {cost_total}   |   FN = {FN}   FP = {FP}   |   Hit@2 = {hit_at_2:.2%}\")\n",
    "\n",
    "# Per-area breakdown\n",
    "def _area_stats(d):\n",
    "    y = d[\"y_true\"].to_numpy(int); p = d[\"pred_peakday\"].to_numpy(int)\n",
    "    tp = int(((y==1) & (p==1)).sum())\n",
    "    fp = int(((y==0) & (p==1)).sum())\n",
    "    fn = int(((y==1) & (p==0)).sum())\n",
    "    return pd.Series({\"Hit@2\": tp, \"FP\": fp, \"FN\": fn, \"Cost\": 4*fn + fp})\n",
    "\n",
    "per_area = (eval_df.groupby(\"load_area\", observed=True)\n",
    "            .apply(_area_stats).sort_values(\"Cost\", ascending=True))\n",
    "print(\"\\nPer-area (Hit@2, FP, FN, Cost):\")\n",
    "print(per_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a9640-b039-4536-9c3e-35aa1bbc2b60",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9306864-673d-40c5-9597-dc2ab19765ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_3207/3908537845.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"ts\"] = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows after leakage sanitization: 2,364,657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 80\u001b[0m\n\u001b[1;32m     65\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     67\u001b[0m hgb \u001b[38;5;241m=\u001b[39m HistGradientBoostingRegressor(\n\u001b[1;32m     68\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 80\u001b[0m hgb\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 6) PREDICT VALIDATION **without using validation actuals**:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#    Roll forward per (area, ts): if a required lag points inside holdout,\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#    use our prior PREDICTION for that lag time (never the actual).\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Known dictionary: all training actuals only\u001b[39;00m\n\u001b[1;32m     88\u001b[0m known \u001b[38;5;241m=\u001b[39m {(r\u001b[38;5;241m.\u001b[39mload_area, r\u001b[38;5;241m.\u001b[39mts): r\u001b[38;5;241m.\u001b[39mmw \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m train_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_area\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmw\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:920\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trees_per_iteration_):\n\u001b[1;32m    901\u001b[0m     grower \u001b[38;5;241m=\u001b[39m TreeGrower(\n\u001b[1;32m    902\u001b[0m         X_binned\u001b[38;5;241m=\u001b[39mX_binned_train,\n\u001b[1;32m    903\u001b[0m         gradients\u001b[38;5;241m=\u001b[39mg_view[:, k],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[1;32m    919\u001b[0m     )\n\u001b[0;32m--> 920\u001b[0m     grower\u001b[38;5;241m.\u001b[39mgrow()\n\u001b[1;32m    922\u001b[0m     acc_apply_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grower\u001b[38;5;241m.\u001b[39mtotal_apply_split_time\n\u001b[1;32m    923\u001b[0m     acc_find_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grower\u001b[38;5;241m.\u001b[39mtotal_find_split_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py:387\u001b[0m, in \u001b[0;36mTreeGrower.grow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplittable_nodes:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_next()\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_shrinkage()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py:490\u001b[0m, in \u001b[0;36mTreeGrower.split_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m node \u001b[38;5;241m=\u001b[39m heappop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplittable_nodes)\n\u001b[1;32m    485\u001b[0m tic \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    486\u001b[0m (\n\u001b[1;32m    487\u001b[0m     sample_indices_left,\n\u001b[1;32m    488\u001b[0m     sample_indices_right,\n\u001b[1;32m    489\u001b[0m     right_child_pos,\n\u001b[0;32m--> 490\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit_indices(node\u001b[38;5;241m.\u001b[39msplit_info, node\u001b[38;5;241m.\u001b[39msample_indices)\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_apply_split_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m    493\u001b[0m depth \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = merged_df.copy()\n",
    "if 'load_area' in globals():\n",
    "    df = df[df[\"load_area\"].isin(load_area)].copy()\n",
    "\n",
    "# timestamps (EPT wall-clock)\n",
    "df[\"ts\"] = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ts\"]).sort_values([\"load_area\", \"ts\"]).reset_index(drop=True)\n",
    "df[\"year\"] = df[\"ts\"].dt.year\n",
    "df[\"hour\"] = df[\"ts\"].dt.hour\n",
    "df[\"doy\"]  = df[\"ts\"].dt.dayofyear  \n",
    "\n",
    "def add_last_year_feats(frame: pd.DataFrame, n_years: int = 4) -> pd.DataFrame:\n",
    "    out = frame\n",
    "    base = (frame[[\"load_area\", \"ts\", \"mw\"]]\n",
    "            .groupby([\"load_area\", \"ts\"], as_index=False)[\"mw\"].mean())  # collapse any DST dupes\n",
    "    for k in range(1, n_years + 1):\n",
    "        prev = base.rename(columns={\"mw\": f\"mw_ly{k}\"}).copy()\n",
    "        prev[\"ts\"] = prev[\"ts\"] + pd.DateOffset(years=k)  # align prior-year load to current timestamp\n",
    "        out = out.merge(prev, on=[\"load_area\", \"ts\"], how=\"left\")\n",
    "    return out\n",
    "\n",
    "df = add_last_year_feats(df, n_years=4)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) Create lag features **by exact timestamp lookup** (not shift),\n",
    "#    so we can sanitize training rows that would reference holdout.\n",
    "# -------------------------\n",
    "def add_lag_by_lookup(frame: pd.DataFrame, hours: int, out_col: str) -> pd.DataFrame:\n",
    "    key = frame[[\"load_area\", \"ts\", \"mw\"]].copy()\n",
    "    key = key.rename(columns={\"mw\": out_col})\n",
    "    key[\"ts\"] = key[\"ts\"] + pd.Timedelta(hours=hours)  # align t-hours -> t\n",
    "    return frame.merge(key, on=[\"load_area\", \"ts\"], how=\"left\")\n",
    "\n",
    "for h, c in [(24, \"mw_lag24\"), (48, \"mw_lag48\"), (168, \"mw_lag168\")]:\n",
    "    df = add_lag_by_lookup(df, hours=h, out_col=c)\n",
    "# Define the 10‑day holdout window\n",
    "HOLD_LO = pd.Timestamp(\"2025-10-22 00:00:00\")\n",
    "HOLD_HI = pd.Timestamp(\"2025-11-01 00:00:00\")  # exclusive\n",
    "is_holdout = (df[\"ts\"].ge(HOLD_LO) & df[\"ts\"].lt(HOLD_HI) & df[\"year\"].eq(2025))\n",
    "\n",
    "# Split (no braces, no dict keys, just a boolean Series aligned to df.index)\n",
    "valid_df = df.loc[is_holdout].copy()\n",
    "train_df = df.loc[~is_holdout].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 4) SANITIZE TRAIN: any lag whose reference time falls inside holdout is set to NaN.\n",
    "#    (Prevents leakage of validation actuals into training features.)\n",
    "# -------------------------\n",
    "for lag_h, lag_col in [(24, \"mw_lag24\"), (48, \"mw_lag48\"), (168, \"mw_lag168\")]:\n",
    "    ref_ts = train_df[\"ts\"] - pd.Timedelta(hours=lag_h)\n",
    "    leak_mask = (ref_ts >= HOLD_LO) & (ref_ts < HOLD_HI)\n",
    "    train_df.loc[leak_mask, lag_col] = np.nan\n",
    "\n",
    "# We require all lag features + mw_ly1..mw_ly4 to exist for training\n",
    "feat_cols = [\"mw_lag24\", \"mw_lag48\", \"mw_lag168\"]\n",
    "train_df = train_df.dropna(subset=feat_cols + [\"mw\"]).copy()\n",
    "\n",
    "print(f\"Train rows after leakage sanitization: {len(train_df):,}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Fit boosting model (no CV; uses internal early stopping on TRAIN only)\n",
    "# -------------------------\n",
    "X_train = train_df[feat_cols].to_numpy()\n",
    "y_train = train_df[\"mw\"].to_numpy()\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    learning_rate=0.05,\n",
    "    max_iter=2000,\n",
    "    early_stopping=False,     # TRAIN-only internal holdout\n",
    "    n_iter_no_change=200,\n",
    "    validation_fraction=0.1, # fraction of TRAIN\n",
    "    max_bins=255,\n",
    "    min_samples_leaf=50,\n",
    "    l2_regularization=1.0,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "hgb.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------\n",
    "# 6) PREDICT VALIDATION **without using validation actuals**:\n",
    "#    Roll forward per (area, ts): if a required lag points inside holdout,\n",
    "#    use our prior PREDICTION for that lag time (never the actual).\n",
    "# -------------------------\n",
    "# Known dictionary: all training actuals only\n",
    "known = {(r.load_area, r.ts): r.mw for r in train_df[[\"load_area\",\"ts\",\"mw\"]].itertuples(index=False)}\n",
    "\n",
    "# Prepare container for predictions\n",
    "valid_df = valid_df.sort_values([\"load_area\", \"ts\"]).copy()\n",
    "valid_df[\"pred\"] = np.nan\n",
    "\n",
    "def row_feat_vector(row, known_map):\n",
    "    area, ts = row[\"load_area\"], row[\"ts\"]\n",
    "    l24  = known_map.get((area, ts - pd.Timedelta(hours=24)),  np.nan)\n",
    "    l48  = known_map.get((area, ts - pd.Timedelta(hours=48)),  np.nan)\n",
    "    l168 = known_map.get((area, ts - pd.Timedelta(hours=168)), np.nan)\n",
    "    return np.array([[l24, l48, l168]])\n",
    "\n",
    "# Roll forward within each area\n",
    "for area, g in valid_df.groupby(\"load_area\", sort=False):\n",
    "    idxs = g.sort_values(\"ts\").index\n",
    "    for idx in idxs:\n",
    "        feats = row_feat_vector(valid_df.loc[idx], known)\n",
    "        yhat  = float(hgb.predict(feats)[0])\n",
    "        valid_df.at[idx, \"pred\"] = yhat\n",
    "        # update knowledge so downstream lags can use our prediction\n",
    "        known[(area, valid_df.at[idx, \"ts\"])] = yhat\n",
    "\n",
    "# -------------------------\n",
    "# 7) Metrics on the 10-day validation window\n",
    "# -------------------------\n",
    "y_valid = valid_df[\"mw\"].to_numpy()\n",
    "y_hat   = valid_df[\"pred\"].to_numpy()\n",
    "\n",
    "print(\"\\nValidation (NO validation info used in train or features): 2025-10-22..2025-10-31\")\n",
    "print(f\"  RMSE: {rmse(y_valid, y_hat):,.2f} MW\")\n",
    "print(f\"  MAE : {mae(y_valid,  y_hat):,.2f} MW\")\n",
    "print(f\"  MAPE: {mape(y_valid, y_hat):,.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9cae7f-b9e2-4e95-88a7-00ab90b064f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_3207/56292226.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"ts\"] = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] rows: 2,244,232  features: ['mw_lag24', 'mw_lag48', 'mw_lag168']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_3207/56292226.py:89: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  roll_hours = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Roll-forward] Predicted 2,784 rows (29 areas × 96 hours).\n",
      "\n",
      "[Nov 20] rows: 696 (areas × 24 hours)\n",
      "load_area                  ts  hour       pred\n",
      "     AECO 2025-11-20 00:00:00     0 820.970155\n",
      "     AECO 2025-11-20 01:00:00     1 798.861627\n",
      "     AECO 2025-11-20 02:00:00     2 786.618239\n",
      "     AECO 2025-11-20 03:00:00     3 786.618239\n",
      "     AECO 2025-11-20 04:00:00     4 794.369574\n",
      "     AECO 2025-11-20 05:00:00     5 826.109342\n",
      "     AECO 2025-11-20 06:00:00     6 876.250631\n",
      "     AECO 2025-11-20 07:00:00     7 876.250631\n",
      "     AECO 2025-11-20 08:00:00     8 820.970155\n",
      "     AECO 2025-11-20 09:00:00     9 769.758397\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_area = [\n",
    "    \"AECO\",\n",
    "    \"AEPAPT\",\n",
    "    \"AEPIMP\",\n",
    "    \"AEPKPT\",\n",
    "    \"AEPOPT\",\n",
    "    \"AP\",\n",
    "    \"BC\",\n",
    "    \"CE\",\n",
    "    \"DAY\",\n",
    "    \"DEOK\",\n",
    "    \"DOM\",\n",
    "    \"DPLCO\",\n",
    "    \"DUQ\",\n",
    "    \"EASTON\",\n",
    "    \"EKPC\",\n",
    "    \"JC\",\n",
    "    \"ME\",\n",
    "    \"OE\",\n",
    "    \"OVEC\",\n",
    "    \"PAPWR\",\n",
    "    \"PE\",\n",
    "    \"PEPCO\",\n",
    "    \"PLCO\",\n",
    "    \"PN\",\n",
    "    \"PS\",\n",
    "    \"RECO\",\n",
    "    \"SMECO\",\n",
    "    \"UGI\",\n",
    "    \"VMEU\"\n",
    "]\n",
    "# -------------------------------\n",
    "# 0) Configuration\n",
    "# -------------------------------\n",
    "# Train on everything before 2025-11-17 00:00 (i.e., up to and including Nov 16)\n",
    "TRAIN_END = pd.Timestamp(\"2025-11-17 00:00:00\")  # exclusive\n",
    "# Roll-forward (consecutive) prediction days (inclusive)\n",
    "ROLL_START_DAY = pd.Timestamp(\"2025-11-17\").normalize()\n",
    "ROLL_END_DAY   = pd.Timestamp(\"2025-11-20\").normalize()   # last day you asked to predict\n",
    "\n",
    "FEATS = [\"mw_lag24\", \"mw_lag48\", \"mw_lag168\"]\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Training set (strictly before TRAIN_END)\n",
    "# -------------------------------\n",
    "\n",
    "df = merged_df.copy()\n",
    "df[\"ts\"] = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"ts\"]).sort_values([\"load_area\", \"ts\"]).reset_index(drop=True)\n",
    "df[\"year\"] = df[\"ts\"].dt.year\n",
    "df[\"hour\"] = df[\"ts\"].dt.hour\n",
    "df[\"doy\"]  = df[\"ts\"].dt.dayofyear \n",
    "df = df[df[\"load_area\"].isin(load_area)].copy()\n",
    "def add_lag_by_lookup(frame: pd.DataFrame, hours: int, out_col: str) -> pd.DataFrame:\n",
    "    key = frame[[\"load_area\", \"ts\", \"mw\"]].copy()\n",
    "    key = key.rename(columns={\"mw\": out_col})\n",
    "    key[\"ts\"] = key[\"ts\"] + pd.Timedelta(hours=hours)  # align t-hours -> t\n",
    "    return frame.merge(key, on=[\"load_area\", \"ts\"], how=\"left\")\n",
    "\n",
    "for h, c in [(24, \"mw_lag24\"), (48, \"mw_lag48\"), (168, \"mw_lag168\")]:\n",
    "    df = add_lag_by_lookup(df, hours=h, out_col=c)\n",
    "\n",
    "\n",
    "train_cut = df.loc[df[\"ts\"] < TRAIN_END].dropna(subset=FEATS + [\"mw\"]).copy()\n",
    "X_tr = train_cut[FEATS].to_numpy()\n",
    "y_tr = train_cut[\"mw\"].to_numpy()\n",
    "\n",
    "hgb_roll = HistGradientBoostingRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    learning_rate=0.05,\n",
    "    max_iter=2000,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=100,\n",
    "    validation_fraction=0.10,\n",
    "    max_bins=255,\n",
    "    min_samples_leaf=50,\n",
    "    l2_regularization=1.0,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "print(f\"[Train] rows: {len(train_cut):,}  features: {FEATS}\")\n",
    "hgb_roll.fit(X_tr, y_tr)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Roll-forward across Nov 17 → 18 → 19 → 20\n",
    "#    Use predictions to populate lag features when needed.\n",
    "# -------------------------------\n",
    "# Build hourly grid we will traverse sequentially\n",
    "roll_hours = pd.date_range(\n",
    "    ROLL_START_DAY,\n",
    "    ROLL_END_DAY + pd.Timedelta(days=1),\n",
    "    freq=\"H\",\n",
    "    inclusive=\"left\"\n",
    ")\n",
    "\n",
    "# Areas to run (use your list if provided; otherwise infer)\n",
    "areas = sorted(df[\"load_area\"].dropna().unique().tolist())\n",
    "\n",
    "# Known map seeded ONLY with training actuals (no leakage)\n",
    "known = {(r.load_area, r.ts): r.mw\n",
    "         for r in train_cut[[\"load_area\",\"ts\",\"mw\"]].itertuples(index=False)}\n",
    "\n",
    "def feat_vec(area, ts, known_map):\n",
    "    \"\"\"Build 1x3 lag vector using ACTUALS from train or prior PREDICTIONS.\"\"\"\n",
    "    l24  = known_map.get((area, ts - pd.Timedelta(hours=24)),  np.nan)\n",
    "    l48  = known_map.get((area, ts - pd.Timedelta(hours=48)),  np.nan)\n",
    "    l168 = known_map.get((area, ts - pd.Timedelta(hours=168)), np.nan)\n",
    "    return np.array([[l24, l48, l168]], dtype=float)\n",
    "\n",
    "rows = []\n",
    "for area in areas:\n",
    "    for ts in roll_hours:\n",
    "        X = feat_vec(area, ts, known)\n",
    "        yhat = float(hgb_roll.predict(X)[0])   # HGB supports NaN in features\n",
    "        rows.append((area, ts, ts.hour, yhat))\n",
    "        # IMPORTANT: immediately make this available as a lag for downstream hours/days\n",
    "        known[(area, ts)] = yhat\n",
    "\n",
    "pred_consecutive = (pd.DataFrame(rows, columns=[\"load_area\",\"ts\",\"hour\",\"pred\"])\n",
    "                      .sort_values([\"load_area\",\"ts\"])\n",
    "                      .reset_index(drop=True))\n",
    "\n",
    "print(f\"[Roll-forward] Predicted {len(pred_consecutive):,} rows \"\n",
    "      f\"({len(areas)} areas × {len(roll_hours)} hours).\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3) (Optional) Metrics where actuals exist in df (for those days)\n",
    "# -------------------------------\n",
    "eval_slice = df.loc[(df[\"ts\"] >= ROLL_START_DAY) & (df[\"ts\"] < ROLL_END_DAY + pd.Timedelta(days=1)),\n",
    "                    [\"load_area\",\"ts\",\"mw\"]]\n",
    "eval_df = pred_consecutive.merge(eval_slice, on=[\"load_area\",\"ts\"], how=\"left\")\n",
    "\n",
    "if eval_df[\"mw\"].notna().any():\n",
    "    def rmse(a,b):\n",
    "        a,b = np.asarray(a), np.asarray(b)\n",
    "        return float(np.sqrt(np.mean((a-b)**2)))\n",
    "    def mae(a,b):\n",
    "        a,b = np.asarray(a), np.asarray(b)\n",
    "        return float(np.mean(np.abs(a-b)))\n",
    "    def mape(a,b):\n",
    "        a,b = np.asarray(a), np.asarray(b)\n",
    "        m = a != 0\n",
    "        return float(np.mean(np.abs((a[m]-b[m])/a[m]))*100.0) if m.any() else np.nan\n",
    "\n",
    "    print(\"\\n[Metrics on Nov 17–20 where actuals exist]\")\n",
    "    print(f\"  RMSE: {rmse(eval_df['mw'].dropna(), eval_df.loc[eval_df['mw'].notna(), 'pred']):,.2f} MW\")\n",
    "    print(f\"  MAE : {mae (eval_df['mw'].dropna(), eval_df.loc[eval_df['mw'].notna(), 'pred']):,.2f} MW\")\n",
    "    print(f\"  MAPE: {mape(eval_df['mw'].dropna(), eval_df.loc[eval_df['mw'].notna(), 'pred']):,.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Convenience slice: predictions for Nov 20 only (24 hours × areas)\n",
    "# -------------------------------\n",
    "pred_2025_11_20 = pred_consecutive[(pred_consecutive[\"ts\"] >= pd.Timestamp(\"2025-11-20\")) &\n",
    "                                   (pred_consecutive[\"ts\"] <  pd.Timestamp(\"2025-11-21\"))] \\\n",
    "                                  .copy() \\\n",
    "                                  .sort_values([\"load_area\",\"ts\"])\n",
    "print(f\"\\n[Nov 20] rows: {len(pred_2025_11_20):,} (areas × 24 hours)\")\n",
    "print(pred_2025_11_20.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4e07d9-d3e0-44ec-9231-0b08c6aa77d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datetime_beginning_ept</th>\n",
       "      <th>nerc_region</th>\n",
       "      <th>mkt_region</th>\n",
       "      <th>zone</th>\n",
       "      <th>load_area</th>\n",
       "      <th>mw</th>\n",
       "      <th>is_verified</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>source_file</th>\n",
       "      <th>ts</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 05:00:00</td>\n",
       "      <td>1/1/2017 12:00:00 AM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>AECO</td>\n",
       "      <td>1017.985</td>\n",
       "      <td>True</td>\n",
       "      <td>6.4</td>\n",
       "      <td>hrl_load_metered_2017.csv</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 06:00:00</td>\n",
       "      <td>1/1/2017 1:00:00 AM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>AECO</td>\n",
       "      <td>964.157</td>\n",
       "      <td>True</td>\n",
       "      <td>6.3</td>\n",
       "      <td>hrl_load_metered_2017.csv</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 07:00:00</td>\n",
       "      <td>1/1/2017 2:00:00 AM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>AECO</td>\n",
       "      <td>921.292</td>\n",
       "      <td>True</td>\n",
       "      <td>5.9</td>\n",
       "      <td>hrl_load_metered_2017.csv</td>\n",
       "      <td>2017-01-01 02:00:00</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 08:00:00</td>\n",
       "      <td>1/1/2017 3:00:00 AM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>AECO</td>\n",
       "      <td>897.977</td>\n",
       "      <td>True</td>\n",
       "      <td>5.8</td>\n",
       "      <td>hrl_load_metered_2017.csv</td>\n",
       "      <td>2017-01-01 03:00:00</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 09:00:00</td>\n",
       "      <td>1/1/2017 4:00:00 AM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>AECO</td>\n",
       "      <td>895.976</td>\n",
       "      <td>True</td>\n",
       "      <td>5.6</td>\n",
       "      <td>hrl_load_metered_2017.csv</td>\n",
       "      <td>2017-01-01 04:00:00</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252857</th>\n",
       "      <td>2025-11-17 00:00:00</td>\n",
       "      <td>11/16/2025 7:00:00 PM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>VMEU</td>\n",
       "      <td>77.289</td>\n",
       "      <td>False</td>\n",
       "      <td>7.6</td>\n",
       "      <td>hrl_load_metered_2025.csv</td>\n",
       "      <td>2025-11-16 19:00:00</td>\n",
       "      <td>2025-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252858</th>\n",
       "      <td>2025-11-17 01:00:00</td>\n",
       "      <td>11/16/2025 8:00:00 PM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>VMEU</td>\n",
       "      <td>76.403</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>hrl_load_metered_2025.csv</td>\n",
       "      <td>2025-11-16 20:00:00</td>\n",
       "      <td>2025-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252859</th>\n",
       "      <td>2025-11-17 02:00:00</td>\n",
       "      <td>11/16/2025 9:00:00 PM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>VMEU</td>\n",
       "      <td>74.414</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>hrl_load_metered_2025.csv</td>\n",
       "      <td>2025-11-16 21:00:00</td>\n",
       "      <td>2025-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252860</th>\n",
       "      <td>2025-11-17 03:00:00</td>\n",
       "      <td>11/16/2025 10:00:00 PM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>VMEU</td>\n",
       "      <td>71.312</td>\n",
       "      <td>False</td>\n",
       "      <td>5.3</td>\n",
       "      <td>hrl_load_metered_2025.csv</td>\n",
       "      <td>2025-11-16 22:00:00</td>\n",
       "      <td>2025-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252861</th>\n",
       "      <td>2025-11-17 04:00:00</td>\n",
       "      <td>11/16/2025 11:00:00 PM</td>\n",
       "      <td>RFC</td>\n",
       "      <td>MIDATL</td>\n",
       "      <td>AE</td>\n",
       "      <td>VMEU</td>\n",
       "      <td>68.660</td>\n",
       "      <td>False</td>\n",
       "      <td>4.8</td>\n",
       "      <td>hrl_load_metered_2025.csv</td>\n",
       "      <td>2025-11-16 23:00:00</td>\n",
       "      <td>2025-11-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2252862 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  datetime_beginning_ept nerc_region mkt_region  \\\n",
       "0       2017-01-01 05:00:00    1/1/2017 12:00:00 AM         RFC     MIDATL   \n",
       "1       2017-01-01 06:00:00     1/1/2017 1:00:00 AM         RFC     MIDATL   \n",
       "2       2017-01-01 07:00:00     1/1/2017 2:00:00 AM         RFC     MIDATL   \n",
       "3       2017-01-01 08:00:00     1/1/2017 3:00:00 AM         RFC     MIDATL   \n",
       "4       2017-01-01 09:00:00     1/1/2017 4:00:00 AM         RFC     MIDATL   \n",
       "...                     ...                     ...         ...        ...   \n",
       "2252857 2025-11-17 00:00:00   11/16/2025 7:00:00 PM         RFC     MIDATL   \n",
       "2252858 2025-11-17 01:00:00   11/16/2025 8:00:00 PM         RFC     MIDATL   \n",
       "2252859 2025-11-17 02:00:00   11/16/2025 9:00:00 PM         RFC     MIDATL   \n",
       "2252860 2025-11-17 03:00:00  11/16/2025 10:00:00 PM         RFC     MIDATL   \n",
       "2252861 2025-11-17 04:00:00  11/16/2025 11:00:00 PM         RFC     MIDATL   \n",
       "\n",
       "        zone load_area        mw  is_verified  temperature_2m  \\\n",
       "0         AE      AECO  1017.985         True             6.4   \n",
       "1         AE      AECO   964.157         True             6.3   \n",
       "2         AE      AECO   921.292         True             5.9   \n",
       "3         AE      AECO   897.977         True             5.8   \n",
       "4         AE      AECO   895.976         True             5.6   \n",
       "...      ...       ...       ...          ...             ...   \n",
       "2252857   AE      VMEU    77.289        False             7.6   \n",
       "2252858   AE      VMEU    76.403        False             6.9   \n",
       "2252859   AE      VMEU    74.414        False             6.1   \n",
       "2252860   AE      VMEU    71.312        False             5.3   \n",
       "2252861   AE      VMEU    68.660        False             4.8   \n",
       "\n",
       "                       source_file                  ts       date  \n",
       "0        hrl_load_metered_2017.csv 2017-01-01 00:00:00 2017-01-01  \n",
       "1        hrl_load_metered_2017.csv 2017-01-01 01:00:00 2017-01-01  \n",
       "2        hrl_load_metered_2017.csv 2017-01-01 02:00:00 2017-01-01  \n",
       "3        hrl_load_metered_2017.csv 2017-01-01 03:00:00 2017-01-01  \n",
       "4        hrl_load_metered_2017.csv 2017-01-01 04:00:00 2017-01-01  \n",
       "...                            ...                 ...        ...  \n",
       "2252857  hrl_load_metered_2025.csv 2025-11-16 19:00:00 2025-11-16  \n",
       "2252858  hrl_load_metered_2025.csv 2025-11-16 20:00:00 2025-11-16  \n",
       "2252859  hrl_load_metered_2025.csv 2025-11-16 21:00:00 2025-11-16  \n",
       "2252860  hrl_load_metered_2025.csv 2025-11-16 22:00:00 2025-11-16  \n",
       "2252861  hrl_load_metered_2025.csv 2025-11-16 23:00:00 2025-11-16  \n",
       "\n",
       "[2252862 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f5a09c-0054-4f76-84de-cf7bfe598dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['mw_lag24', 'mw_lag48', 'mw_lag168'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m1.4826\u001b[39m \u001b[38;5;241m*\u001b[39m mad)\n\u001b[1;32m     11\u001b[0m train_df \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m---> 12\u001b[0m X_tr \u001b[38;5;241m=\u001b[39m train_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmw_lag24\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmw_lag48\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmw_lag168\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     13\u001b[0m y_tr \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     14\u001b[0m train_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['mw_lag24', 'mw_lag48', 'mw_lag168'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def robust_sigma(s):\n",
    "    med = np.median(s)\n",
    "    mad = np.median(np.abs(s - med))\n",
    "    if np.isnan(mad) or mad == 0:\n",
    "        std = np.std(s)\n",
    "        return float(std if std > 1e-6 else 1.0)\n",
    "    return float(1.4826 * mad)\n",
    "train_df = df\n",
    "X_tr = train_df[[\"mw_lag24\", \"mw_lag48\", \"mw_lag168\"]].to_numpy()\n",
    "y_tr = train_df[\"mw\"].to_numpy()\n",
    "train_df = train_df.copy()\n",
    "train_df[\"pred_tr\"] = hgb_roll.predict(X_tr)\n",
    "train_df[\"resid\"] = train_df[\"mw\"] - train_df[\"pred_tr\"]\n",
    "\n",
    "sigma_map = (train_df.groupby([\"load_area\",\"hour\"], observed=True)[\"resid\"]\n",
    "                        .apply(robust_sigma))\n",
    "sigma_hour_global = (train_df.groupby(\"hour\", observed=True)[\"resid\"]\n",
    "                               .apply(robust_sigma))\n",
    "\n",
    "def get_sigma_vec(area, hours):\n",
    "    out = []\n",
    "    for h in hours:\n",
    "        key = (area, int(h))\n",
    "        if key in sigma_map.index:\n",
    "            out.append(float(sigma_map.loc[key]))\n",
    "        else:\n",
    "            out.append(float(sigma_hour_global.loc[int(h)]))\n",
    "    return np.array(out, dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# B) Roll-forward predictions: Nov 17 → 18 → 19 → 20\n",
    "#    (lags inside this window come from our own predictions)\n",
    "# ============================================================\n",
    "ROLL_START_DAY = pd.Timestamp(\"2025-11-17\")\n",
    "ROLL_END_DAY   = pd.Timestamp(\"2025-11-20\")\n",
    "FEATS = [\"mw_lag24\", \"mw_lag48\", \"mw_lag168\"]\n",
    "\n",
    "# Build hourly grid (inclusive of days, exclusive of end midnight)\n",
    "roll_hours = pd.date_range(ROLL_START_DAY, ROLL_END_DAY + pd.Timedelta(days=1),\n",
    "                           freq=\"H\", inclusive=\"left\")\n",
    "\n",
    "areas = sorted(df[\"load_area\"].dropna().unique().tolist())\n",
    "\n",
    "# Seed known map with ACTUALS strictly before the roll window (no leakage)\n",
    "known = {(r.load_area, r.ts): r.mw\n",
    "         for r in df.loc[df[\"ts\"] < ROLL_START_DAY, [\"load_area\",\"ts\",\"mw\"]]\n",
    "                   .itertuples(index=False)}\n",
    "\n",
    "def feat_vec(area, ts, known_map):\n",
    "    l24  = known_map.get((area, ts - pd.Timedelta(hours=24)),  np.nan)\n",
    "    l48  = known_map.get((area, ts - pd.Timedelta(hours=48)),  np.nan)\n",
    "    l168 = known_map.get((area, ts - pd.Timedelta(hours=168)), np.nan)\n",
    "    return np.array([[l24, l48, l168]], dtype=float)\n",
    "\n",
    "rows = []\n",
    "for area in areas:\n",
    "    for ts in roll_hours:\n",
    "        X1 = feat_vec(area, ts, known)\n",
    "        yhat = float(hgb_roll.predict(X1)[0])  # HGB supports NaN in features\n",
    "        rows.append((area, ts, ts.hour, yhat))\n",
    "        # Make the new prediction available to downstream lags\n",
    "        known[(area, ts)] = yhat\n",
    "\n",
    "pred_consec = (pd.DataFrame(rows, columns=[\"load_area\",\"ts\",\"hour\",\"pred\"])\n",
    "                 .sort_values([\"load_area\",\"ts\"])\n",
    "                 .reset_index(drop=True))\n",
    "\n",
    "# Extract Nov 20 (24 hours × areas)\n",
    "pred_2025_11_20 = pred_consec[(pred_consec[\"ts\"] >= pd.Timestamp(\"2025-11-20\")) &\n",
    "                              (pred_consec[\"ts\"] <  pd.Timestamp(\"2025-11-21\"))] \\\n",
    "                             .copy()\n",
    "\n",
    "# ============================================================\n",
    "# C) Peak-hour selectors (same as your Task-2 block)\n",
    "# ============================================================\n",
    "def soft_argmax(mu, kernel=(1,3,1)):\n",
    "    k = np.array(kernel, dtype=float)\n",
    "    sm = np.convolve(mu, k, mode=\"same\")\n",
    "    return int(np.argmax(sm))\n",
    "\n",
    "def probabilistic_peak(mu, sigma, n_draws=2000, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    Z = rng.standard_normal((n_draws, len(mu)))\n",
    "    Y = mu[None, :] + sigma[None, :] * Z\n",
    "    hmax = np.argmax(Y, axis=1)\n",
    "    votes = np.zeros(len(mu), dtype=int)\n",
    "    for h in hmax:\n",
    "        votes[h] += 1\n",
    "        if h - 1 >= 0: votes[h - 1] += 1\n",
    "        if h + 1 < len(mu): votes[h + 1] += 1\n",
    "    return int(np.argmax(votes))\n",
    "\n",
    "# ============================================================\n",
    "# D) Per-area peak hour for Nov 20 + SAVE\n",
    "# ============================================================\n",
    "p20 = pred_2025_11_20.copy()\n",
    "p20[\"date\"] = p20[\"ts\"].dt.normalize()\n",
    "p20[\"hour\"] = p20[\"hour\"].astype(int)\n",
    "\n",
    "rows = []\n",
    "for area, g in p20.groupby(\"load_area\", sort=True):\n",
    "    gg = g.sort_values(\"hour\")\n",
    "    # If fewer than 24 hours, still proceed (or `continue` if you prefer strictness)\n",
    "    mu  = gg[\"pred\"].to_numpy()\n",
    "    sig = get_sigma_vec(area, gg[\"hour\"].to_numpy())\n",
    "\n",
    "    idx_arg  = int(np.argmax(mu))\n",
    "    idx_soft = soft_argmax(mu, kernel=(1,3,1))\n",
    "    idx_prob = probabilistic_peak(mu, sig, n_draws=2000)\n",
    "\n",
    "    # Map argmax indices back to *hour labels* present in gg\n",
    "    peak_arg = int(gg[\"hour\"].iloc[idx_arg])\n",
    "    peak_soft= int(gg[\"hour\"].iloc[idx_soft])\n",
    "    peak_prob= int(gg[\"hour\"].iloc[idx_prob])\n",
    "\n",
    "    rows.append({\n",
    "        \"load_area\": area,\n",
    "        \"date\":      pd.Timestamp(\"2025-11-20\").date(),\n",
    "        \"peak_hour_argmax\": peak_arg,\n",
    "        \"peak_hour_soft\":   peak_soft,\n",
    "        \"peak_hour_prob\":   peak_prob\n",
    "    })\n",
    "\n",
    "peak_hours_2025_11_20 = (pd.DataFrame(rows)\n",
    "                         .sort_values(\"load_area\")\n",
    "                         .reset_index(drop=True))\n",
    "peak_hours_2025_11_20[\"peak_hour_submit\"] = peak_hours_2025_11_20[\"peak_hour_prob\"]\n",
    "peak_hours_pred = peak_hours_2025_11_20[\"peak_hour_prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83117f59-e207-4f00-a087-3d583d69e119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     18\n",
       "1     19\n",
       "2     19\n",
       "3      8\n",
       "4     19\n",
       "5     18\n",
       "6     18\n",
       "7     18\n",
       "8     18\n",
       "9     18\n",
       "10     7\n",
       "11    18\n",
       "12    18\n",
       "13    18\n",
       "14    19\n",
       "15    18\n",
       "16    18\n",
       "17    18\n",
       "18    15\n",
       "19    18\n",
       "20    18\n",
       "21    18\n",
       "22    18\n",
       "23    17\n",
       "24    18\n",
       "25    17\n",
       "26    19\n",
       "27    18\n",
       "28    17\n",
       "Name: peak_hour_prob, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_hours_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5489ed6-5f90-4702-8c72-d392165d7516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l0/n_657n7d2xdb6sz6mnqb_5zh0000gr/T/ipykernel_3207/831815034.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"ts\"]   = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted peak days (1=yes) between 2025-11-20 and 2025-11-29:\n",
      "    load_area       date  mw_peak  pred_peakday\n",
      "0        AECO 2025-11-20      NaN             0\n",
      "1        AECO 2025-11-21      NaN             1\n",
      "2        AECO 2025-11-22      NaN             1\n",
      "3        AECO 2025-11-23      NaN             0\n",
      "4        AECO 2025-11-24      NaN             0\n",
      "..        ...        ...      ...           ...\n",
      "285      VMEU 2025-11-25      NaN             1\n",
      "286      VMEU 2025-11-26      NaN             1\n",
      "287      VMEU 2025-11-27      NaN             1\n",
      "288      VMEU 2025-11-28      NaN             0\n",
      "289      VMEU 2025-11-29      NaN             0\n",
      "\n",
      "[290 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Daily peak table from merged_df\n",
    "# ------------------------------\n",
    "df = merged_df.copy()\n",
    "if 'load_area' in globals():\n",
    "    df = df[df[\"load_area\"].isin(load_area)].copy()\n",
    "\n",
    "df[\"ts\"]   = pd.to_datetime(df[\"datetime_beginning_ept\"], errors=\"coerce\")\n",
    "df         = df.dropna(subset=[\"ts\"]).sort_values([\"load_area\",\"ts\"]).reset_index(drop=True)\n",
    "df[\"date\"] = df[\"ts\"].dt.normalize()\n",
    "\n",
    "# Actual *daily* peak MW per area (for past years; 2025 might be missing/future)\n",
    "daily = (df.groupby([\"load_area\",\"date\"], observed=True)[\"mw\"]\n",
    "           .max().rename(\"mw_peak\").reset_index())\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Configuration\n",
    "# ------------------------------\n",
    "K_PAST   = 3                                   # use top-3 positions from the corresponding 2024 week\n",
    "WIN_START = pd.Timestamp(\"2025-11-20\").normalize()\n",
    "WIN_END   = pd.Timestamp(\"2025-11-30\").normalize()  # exclusive upper bound (→ up to 11/29)\n",
    "\n",
    "# Helper: Monday-start week for any date\n",
    "def week_start_monday(dt):\n",
    "    return pd.Timestamp(dt).to_period(\"W-MON\").start_time.normalize()\n",
    "\n",
    "# All 2025 dates in the requested window\n",
    "target_dates_2025 = pd.date_range(WIN_START, WIN_END - pd.Timedelta(days=1), freq=\"D\")\n",
    "\n",
    "# Unique Monday-start week anchors that intersect the window\n",
    "week_starts_2025 = sorted({week_start_monday(d) for d in target_dates_2025})\n",
    "\n",
    "pred_rows = []\n",
    "\n",
    "for W25_START in week_starts_2025:\n",
    "    W25_END = W25_START + pd.Timedelta(days=7)\n",
    "    # Corresponding week in 2024 (same weekday alignment)\n",
    "    W24_START = W25_START - pd.DateOffset(years=1)\n",
    "    W24_END   = W24_START + pd.Timedelta(days=7)\n",
    "\n",
    "    # 2024 week (we *need* this week to derive Top-K positions)\n",
    "    d24 = daily[(daily[\"date\"] >= W24_START) & (daily[\"date\"] < W24_END)].copy()\n",
    "    if d24.empty:\n",
    "        # No 2024 data for this aligned week → skip\n",
    "        continue\n",
    "\n",
    "    # Areas observed in the 2024 week\n",
    "    areas = sorted(d24[\"load_area\"].unique())\n",
    "\n",
    "    # Build the full 7-day grid for the 2025 aligned week (even if 2025 actuals are missing)\n",
    "    week25_days = [W25_START + pd.Timedelta(days=i) for i in range(7)]\n",
    "    grid = pd.MultiIndex.from_product([areas, week25_days], names=[\"load_area\",\"date\"]).to_frame(index=False)\n",
    "\n",
    "    # Week positions 0..6 for both weeks\n",
    "    map_pos_24 = pd.DataFrame({\n",
    "        \"date\": [W24_START + pd.Timedelta(days=i) for i in range(7)],\n",
    "        \"pos\":  np.arange(7)\n",
    "    })\n",
    "    map_pos_25 = pd.DataFrame({\n",
    "        \"date\": [W25_START + pd.Timedelta(days=i) for i in range(7)],\n",
    "        \"pos\":  np.arange(7)\n",
    "    })\n",
    "\n",
    "    # Attach positions\n",
    "    d24  = d24.merge(map_pos_24, on=\"date\", how=\"left\")\n",
    "    grid = grid.merge(map_pos_25, on=\"date\", how=\"left\")\n",
    "\n",
    "    # Top-K positions (by daily peak) from 2024 week per area\n",
    "    topK_2024 = (d24.sort_values([\"load_area\",\"mw_peak\"], ascending=[True, False])\n",
    "                   .groupby(\"load_area\", as_index=False).head(K_PAST)\n",
    "                   [[\"load_area\",\"pos\"]]\n",
    "                   .groupby(\"load_area\")[\"pos\"]\n",
    "                   .apply(lambda s: sorted(pd.unique(s)))\n",
    "                   .rename(\"pos_set\").reset_index())\n",
    "\n",
    "    # Predict: positions that appeared among last year's Top-K\n",
    "    grid = grid.merge(topK_2024, on=\"load_area\", how=\"left\")\n",
    "    grid[\"pred_peakday\"] = grid.apply(\n",
    "        lambda r: int(isinstance(r[\"pos_set\"], list) and (r[\"pos\"] in r[\"pos_set\"])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # (Optional) attach 2025 actual daily peaks if they exist in your data\n",
    "    grid = grid.merge(daily[[\"load_area\",\"date\",\"mw_peak\"]], on=[\"load_area\",\"date\"], how=\"left\")\n",
    "\n",
    "    # Keep only the requested window dates\n",
    "    grid = grid[(grid[\"date\"] >= WIN_START) & (grid[\"date\"] < WIN_END)].copy()\n",
    "\n",
    "    pred_rows.append(grid[[\"load_area\",\"date\",\"mw_peak\",\"pred_peakday\"]])\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Final predictions for 2025-11-20 ... 2025-11-29\n",
    "# ------------------------------\n",
    "if pred_rows:\n",
    "    pred_peakdays_2025 = (pd.concat(pred_rows, ignore_index=True)\n",
    "                            .sort_values([\"load_area\",\"date\"])\n",
    "                            .reset_index(drop=True))\n",
    "else:\n",
    "    pred_peakdays_2025 = pd.DataFrame(columns=[\"load_area\",\"date\",\"mw_peak\",\"pred_peakday\"])\n",
    "\n",
    "print(\"Predicted peak days (1=yes) between 2025-11-20 and 2025-11-29:\")\n",
    "print(pred_peakdays_2025)\n",
    "\n",
    "peakday_pred = pred_peakdays_2025[pred_peakdays_2025[\"date\"] == \"2025-11-20\"][\"pred_peakday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "310e5890-fe1c-4cbf-8d01-7ba8a846463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "10     0\n",
       "20     1\n",
       "30     0\n",
       "40     1\n",
       "50     0\n",
       "60     0\n",
       "70     1\n",
       "80     1\n",
       "90     1\n",
       "100    0\n",
       "110    0\n",
       "120    0\n",
       "130    0\n",
       "140    0\n",
       "150    0\n",
       "160    0\n",
       "170    1\n",
       "180    1\n",
       "190    0\n",
       "200    0\n",
       "210    0\n",
       "220    0\n",
       "230    1\n",
       "240    1\n",
       "250    1\n",
       "260    0\n",
       "270    0\n",
       "280    0\n",
       "Name: pred_peakday, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37ef624e-0e1d-4d2c-863d-5610b43cf460",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peak_hours_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Row count per area\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m first_two \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(pred_2025_11_20[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m],peak_hours_pred)\n\u001b[1;32m      4\u001b[0m df_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-11-20\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mappend(peakday_pred)}\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m df_out\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_2025_11_20.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peak_hours_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# 1) Row count per area\n",
    "first_two = np.append(pred_2025_11_20[\"pred\"],peak_hours_pred)\n",
    "\n",
    "df_out = pd.DataFrame(\n",
    "    {\"2025-11-20\": np.append(peakday_pred)}\n",
    ")\n",
    "\n",
    "df_out.to_csv(\"pred_2025_11_20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3ce37d-9c6d-406f-901b-e153e1324347",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_out)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_out' is not defined"
     ]
    }
   ],
   "source": [
    "print(df_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
